version: '3.8'

services:
  inference:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: diagnosethat-inference
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      # Supabase
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_SERVICE_ROLE_KEY=${SUPABASE_SERVICE_ROLE_KEY}
      # Cloudflare Queues
      - CF_ACCOUNT_ID=${CF_ACCOUNT_ID}
      - CF_QUEUE_ID=${CF_QUEUE_ID:-inference-jobs}
      - CF_QUEUES_TOKEN=${CF_QUEUES_TOKEN}
      # Worker settings
      - WORKER_ID=${WORKER_ID:-worker-1}
      - POLL_INTERVAL=${POLL_INTERVAL:-0.25}
      - MAX_BACKOFF=${MAX_BACKOFF:-5.0}
      - VISIBILITY_TIMEOUT_MS=${VISIBILITY_TIMEOUT_MS:-300000}
      - BATCH_SIZE=${BATCH_SIZE:-5}
      - MODEL_PATH=/app/models
    volumes:
      - ./models:/app/models:ro
      - inference-cache:/root/.cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

volumes:
  inference-cache:
